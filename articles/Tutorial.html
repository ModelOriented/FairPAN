<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>FairPAN Tutorial • fairpan</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- dalexverse --><link href="../dalexverse.css" rel="stylesheet">
<link href="../dalexverse-2.css" rel="stylesheet">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="FairPAN Tutorial">
<meta property="og:description" content="fairpan">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- google analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-5650686-14"></script><script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'UA-5650686-14');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <div class="navbar-brand-container">
        <a class="navbar-brand" href="../index.html">fairpan</a>
        <div class="info">
          <span class="partof">part of the <a href="https://github.com/ModelOriented/DrWhy">DrWhy.AI</a>
           developed by the <a href="https://mi2.mini.pw.edu.pl/">MI^2 DataLab</a> </span>
          <span class="version version-default">0.1.0</span>
        </div>
      </div>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav navbar-right">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Tutorial.html">FairPAN Tutorial</a>
    </li>
  </ul>
</li>
        
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="Tutorial_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>FairPAN Tutorial</h1>
                        <h4 class="author">Hubert Ruczyński</h4>
            
      
      
      <div class="hidden name"><code>Tutorial.Rmd</code></div>

    </div>

    
    
<div id="fairpan" class="section level1">
<h1 class="hasAnchor">
<a href="#fairpan" class="anchor"></a>FairPAN</h1>
<p>In this tutorial you will get to know when, why and how to use <code>FairPAN</code> and you will also learn about how this model works from theoretical side. <code>FairPAN</code>, which stands for <code>Fair Predictive Adversarial Network</code>, is a tool not only for creating fair predictive adversarial networks, but also for monitoring the training process and visualizing its outcomes. This package is really flexible, because the user can not only provide its own neural network models, but also create them with our functions.</p>
<div id="how-does-it-work" class="section level2">
<h2 class="hasAnchor">
<a href="#how-does-it-work" class="anchor"></a>How does it work?</h2>
<div id="introduction-to-fairness" class="section level3">
<h3 class="hasAnchor">
<a href="#introduction-to-fairness" class="anchor"></a>Introduction to Fairness</h3>
<p>Consider the idea of the algorithm that has to predict whether giving credit to a person is risky or not. It is learning on real data of giving credits which were biased against females (historical fact). In that case, the model learns this bias, which is not only included in the simple sex variable but also is hidden inside other variables. Fairness enables us to detect such bias and handles a few methods to fight it. To learn more, I recommend the article <a href="%22https://arxiv.org/pdf/2104.00507.pdf%22">‘Fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation’ by Jakub Wisniewski and Przemysław Biecek</a>.</p>
</div>
<div id="introduction-to-gans" class="section level3">
<h3 class="hasAnchor">
<a href="#introduction-to-gans" class="anchor"></a>Introduction to GANs</h3>
<p>Generative Adversarial Networks are two neural networks that learn together. The Generator has to generate new samples that are indistinguishable from original data and the adversarial has to distinguish if the observation is original or generated. The generator is punished whenever the adversarial makes the correct prediction. After such process generator eventually learns how to make indistinguishable predictions and adversaries’ accuracy drops up to 50% when a model cannot distinguish the two classes. The idea of GANs was proposed in <a href="https://arxiv.org/pdf/1406.2661.pdf">Generative Adversarial Nets, Ian Goodfellow</a>.</p>
</div>
<div id="fairpan-1" class="section level3">
<h3 class="hasAnchor">
<a href="#fairpan-1" class="anchor"></a>FairPAN</h3>
<p>FairPANs are the solution to bring fairness into neural networks. We mimic the GANs by subsetting generator with classifier (predictor) and adversarial has to predict the sensitive value (such as sex, race, etc) from the output of the predictor. This process eventually leads the classifier to make predictions with indistinguishable sensitive values. The idea comes from blogs: <a href="https://godatadriven.com/blog/towards-fairness-in-ml-with-adversarial-networks/">Towards fairness in ML with adversarial networks, Stijn Tonk</a> and <a href="https://godatadriven.com/blog/fairness-in-machine-learning-with-pytorch/">Fairness in Machine Learning with PyTorch, Henk Griffoen</a> however, our implementation in R offers slightly different solutions. And the exact idea behind using GANs for Fairness is described in <a href="https://stanford.edu/~cpiech/bio/papers/fairnessAdversary.pdf">Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction, Christina Wadsworth, Francesca Vera, Chris Piech</a>.</p>
<p><img src="../man/images/architecture_PAN.png"></p>
<p>The diagram above represents the architecture of our model and is strongly inspired by aforementioned blogs</p>
</div>
<div id="custom-loss-function" class="section level3">
<h3 class="hasAnchor">
<a href="#custom-loss-function" class="anchor"></a>Custom Loss Function</h3>
<p>The crucial part of this model is the metric we use to engage the two models into a zero-sum game. This is captured by the following objective function:</p>
<p><img src="../man/images/equation.png" style="width:50.0%"></p>
<p>So, it learns to minimize its prediction losses while maximizing that of the adversarial (due to lambda being positive and minimizing a negated loss is the same as maximizing it). The objective during the game is simpler for the adversarial: predict sex based on the income level predictions of the classifier. This is captured in the following objective function:</p>
<p><img src="../man/images/equation2.png" style="width:30.0%"></p>
<p>The adversarial does not care about the prediction accuracy of the classifier. It is only concerned with minimizing its prediction losses. Firstly we pretrain classifier and adversarial. Later we begin the proper PAN training with both networks: we train the adversarial, provide its loss to the classifier, and after that, we train the classifier. This method shall lead us to fair predictions of the FairPAN model.</p>
</div>
</div>
<div id="why" class="section level2">
<h2 class="hasAnchor">
<a href="#why" class="anchor"></a>Why?</h2>
<p>Regular mitigation techniques tend to worsen performance of the classifier a lot by decreasing accuracy for example, whereas FairPAN has no such drawback and worsening of the performance is really small. Moreover, our package is very flexible because it enables to provide your own neural networks, but also to create one with our functions. The outcomes are also created with the usage of <code>DALEX</code> and <code>fairmodels</code>, so one can use their methods and visualizations. Additionally the workflow of the package is really simple and clean, because of multiple features available for user, such as <code>preprocess</code> function.</p>
</div>
<div id="data" class="section level2">
<h2 class="hasAnchor">
<a href="#data" class="anchor"></a>Data</h2>
<p>The dataset used in our tutorial is called adult. It contains 15 columns with both numerical and categorical data about citizens of USA. Our target here is a <code>salary</code> column. As sensitive variables we can perceive <code>sex</code> and <code>race</code> however we will focus only on <code>sex</code>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">fairpan</span><span class="op">)</span></code></pre></div>
<pre><code>## Loading required package: torch</code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://fairmodels.drwhy.ai/">fairmodels</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"adult"</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">adult</span><span class="op">)</span></code></pre></div>
<pre><code>##   salary age        workclass fnlwgt education education_num     marital_status
## 1  &lt;=50K  39        State-gov  77516 Bachelors            13      Never-married
## 2  &lt;=50K  50 Self-emp-not-inc  83311 Bachelors            13 Married-civ-spouse
## 3  &lt;=50K  38          Private 215646   HS-grad             9           Divorced
## 4  &lt;=50K  53          Private 234721      11th             7 Married-civ-spouse
## 5  &lt;=50K  28          Private 338409 Bachelors            13 Married-civ-spouse
## 6  &lt;=50K  37          Private 284582   Masters            14 Married-civ-spouse
##          occupation  relationship  race    sex capital_gain capital_loss
## 1      Adm-clerical Not-in-family White   Male         2174            0
## 2   Exec-managerial       Husband White   Male            0            0
## 3 Handlers-cleaners Not-in-family White   Male            0            0
## 4 Handlers-cleaners       Husband Black   Male            0            0
## 5    Prof-specialty          Wife Black Female            0            0
## 6   Exec-managerial          Wife White Female            0            0
##   hours_per_week native_country
## 1             40  United-States
## 2             13  United-States
## 3             40  United-States
## 4             40  United-States
## 5             40           Cuba
## 6             40  United-States</code></pre>
</div>
</div>
<div id="workflow" class="section level1">
<h1 class="hasAnchor">
<a href="#workflow" class="anchor"></a>Workflow</h1>
<div id="preprocessing" class="section level2">
<h2 class="hasAnchor">
<a href="#preprocessing" class="anchor"></a>Preprocessing</h2>
<p>At the beginning we have to preprocess our dataset so we can train a neural network on it and divide it into train and test subsets. One can do it on your own, however we will use built in function called preprocess which will create 16 objects which are needed for other features.</p>
<p>To use this function we have to provide a dataset with categorical columns provided as factors. Then we define that <code>salary</code> is our target and <code>sex</code> is a sensitive variable with privileged level <code>Male</code> and discriminated <code>Female</code>. As we noticed before, <code>race</code> could also be considered as a sensitive variable so we want to remove it from our learning dataset too. In the end we sample a small part of the dataset for our process to save our time in this example and define proportions of train, test and validation subsets. We also set seed for reproduction.</p>
<p>Inside, the function encodes the categorical columns as integers based on their factor levels. After that all variables are rescaled to ensure better learning process. Moreover to ensure that adversarial model works properly, we also balance a dataset to have the same number of privileged and discriminated records.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/preprocess.html">preprocess</a></span><span class="op">(</span> data <span class="op">=</span> <span class="va">adult</span>,               <span class="co"># dataset </span>
                    target_name <span class="op">=</span> <span class="st">"salary"</span>,     <span class="co"># name of target column</span>
                    sensitive_name <span class="op">=</span> <span class="st">"sex"</span>,     <span class="co"># name of sensitive column</span>
                    privileged <span class="op">=</span> <span class="st">"Male"</span>,        <span class="co"># level of privileged class</span>
                    discriminated <span class="op">=</span> <span class="st">"Female"</span>,   <span class="co"># level of discriminated class</span>
                    drop_also <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"race"</span><span class="op">)</span>,      <span class="co"># columns to drop (perhaps </span>
                                                <span class="co"># other sensitive variable)</span>
                    sample <span class="op">=</span> <span class="fl">0.02</span>,              <span class="co"># sample size from dataset</span>
                    train_size <span class="op">=</span> <span class="fl">0.6</span>,           <span class="co"># size of train set</span>
                    test_size <span class="op">=</span> <span class="fl">0.4</span>,            <span class="co"># size of test set</span>
                    validation_size <span class="op">=</span> <span class="fl">0</span>,        <span class="co"># size of validation set</span>
                    seed <span class="op">=</span> <span class="fl">7</span>                    <span class="co"># seed for reproduction.</span>
<span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">train_x</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>##            [,1]      [,2]        [,3]       [,4]    [,5]       [,6]      [,7]
## [1,] -0.3268045 0.1818740 -0.66422374 -0.3411161 1.24547 -0.4622176 -0.442901
## [2,]  2.2741900 0.8282032 -0.05688447 -0.3411161 1.24547  1.4187309 -0.442901
##            [,8]       [,9]      [,10]      [,11]     [,12]
## [1,] -1.0684260 -0.1091987 -0.2235767 0.48039857 0.2780602
## [2,] -0.4861406 -0.1091987 -0.2235767 0.06808044 0.2780602</code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">train_y</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1 1</code></pre>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">sensitive_train</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 2 2</code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">test_x</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>##            [,1]      [,2]       [,3]      [,4]       [,5]       [,6]       [,7]
## [1,] -1.3383024 -1.110784 -0.1091827 1.1762624 -0.0216091  0.7917481  0.4460018
## [2,] -0.9048033  0.181874 -0.2782588 0.1646767 -0.4439688 -0.4622176 -1.1095780
##           [,8]       [,9]      [,10]      [,11]     [,12]
## [1,] 0.6784302 -0.1091987 -0.2235767 -0.3442377 0.2780602
## [2,] 1.8430009 -0.1091987 -0.2235767  0.8927167 0.2780602</code></pre>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">test_y</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1 2</code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">sensitive_test</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1 1</code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">data_scaled_test</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>##            [,1]      [,2]       [,3]      [,4]       [,5]       [,6]       [,7]
## [1,] -1.3383024 -1.110784 -0.1091827 1.1762624 -0.0216091  0.7917481  0.4460018
## [2,] -0.9048033  0.181874 -0.2782588 0.1646767 -0.4439688 -0.4622176 -1.1095780
##           [,8]       [,9]      [,10]      [,11]     [,12]
## [1,] 0.6784302 -0.1091987 -0.2235767 -0.3442377 0.2780602
## [2,] 1.8430009 -0.1091987 -0.2235767  0.8927167 0.2780602</code></pre>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">data_test</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>##       salary age workclass fnlwgt    education education_num     marital_status
## 23812  &lt;=50K  19 Local-gov 176831 Some-college            10      Never-married
## 10859   &gt;50K  25   Private 158662      HS-grad             9 Married-civ-spouse
##          occupation relationship  race    sex capital_gain capital_loss
## 23812 Other-service    Own-child Black Female            0            0
## 10859  Adm-clerical         Wife White Female            0            0
##       hours_per_week native_country
## 23812             35  United-States
## 10859             50  United-States</code></pre>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">protected_test</span>,<span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] Female Female
## Levels: Female Male</code></pre>
<p>We’ve decided to show the most important objects created by preprocess above.</p>
<p>Our next step is setting a computational device <code>dev</code>, which might be GPU with CUDA or cpu if we don’t have CUDA installed. Even more importantly we create a dataset_loader object which stores data as tensor for our neural network in 4 objects. First two of them are torch <code>datasets</code> for storing all the tensors and the other two are torch <code>dataloaders</code> which store tensors in batches described by <code>batch_size</code>.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">dev</span> <span class="op">&lt;-</span>  <span class="st">"cpu"</span>

<span class="va">dsl</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dataset_loader.html">dataset_loader</a></span><span class="op">(</span>train_x <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_x</span>,
                      train_y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_y</span>,
                      test_x <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">test_x</span>,
                      test_y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">test_y</span>,
                      batch_size <span class="op">=</span> <span class="fl">5</span>,
                      dev <span class="op">=</span> <span class="va">dev</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">dsl</span><span class="op">$</span><span class="va">train_dl</span><span class="op">$</span><span class="fu">.iter</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">.next</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## $x_cont
## torch_tensor
## Columns 1 to 10-0.3268  0.1819 -0.6642 -0.3411  1.2455 -0.4622 -0.4429 -1.0684 -0.1092 -0.2236
##  2.2742  0.8282 -0.0569 -0.3411  1.2455  1.4187 -0.4429 -0.4861 -0.1092 -0.2236
## -0.3991  0.1819  1.2816 -0.3411  1.2455  0.7917  0.4460  1.2607 -0.1092 -0.2236
##  0.6124 -1.7571  0.1718 -0.3411  1.2455 -0.4622 -0.4429 -1.0684  0.9980 -0.2236
##  0.9014  0.1819 -0.4723  0.1647 -0.4440 -0.4622  1.7794 -1.0684 -0.1092 -0.2236
## 
## Columns 11 to 12 0.4804  0.2781
##  0.0681  0.2781
##  0.0681  0.2781
##  0.0681  0.2781
##  1.7174  0.2781
## [ CPUFloatType{5,12} ]
## 
## $y
## torch_tensor
##  1
##  1
##  1
##  2
##  2
## [ CPULongType{5} ]</code></pre>
<p>In the end of preprocessing you can see how the single batch of train data loader looks like.</p>
</div>
<div id="model-creation-and-pretrain" class="section level2">
<h2 class="hasAnchor">
<a href="#model-creation-and-pretrain" class="anchor"></a>Model creation and pretrain</h2>
<p>Finally we are ready to create and pretrain both adversarial and classifier models. <code>FairPAN</code> provides multiple options in this case, because one can not only create and pretrain both models with our interface, but also provide their own neural network models (<code>clf_model</code> and <code>adv_model</code>). The classifier model can be also pretrained, but then <code>clf_optimizer</code> from that training must be provided and <code>trained</code> changed to TRUE.</p>
<p>In our first example we will focus on pretraining both models created by the <code>pretrain</code> function. To do that you don’t have to provide first four variables, because they are set like that on default. One has to provide <code>data</code> in next four variables and then, the other two describe inner <code>dataset_loader</code> for adversarial network. Next block describes the structure of our models. Neural architecture is provided as a simple vector where, for example, c(32,16,8) describes a network with 3 layers with 32, 16 and 8 neurons. This layers are connected with <code>nn_linear</code> and during forward pass, inner layers have <code>nnf_relu</code> with <code>nnf_softmax</code> in the end. User can also define dimension for softmax, however we advice not to do so, because other approaches are not sufficiently tested. In the end one can also provide learning rates and numbers of epochs for both models. In the end we have to provide a dataset_loader with device and choose whether we want to monitor more metrics and print them out. As a result we will obtain pretrained neural network models (classifier and adversarial) and optimizers for those two models.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">models</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pretrain.html">pretrain</a></span><span class="op">(</span>clf_model <span class="op">=</span> <span class="cn">NULL</span>,                       <span class="co"># classifier model</span>
                   adv_model <span class="op">=</span> <span class="cn">NULL</span>,                       <span class="co"># adversarial model</span>
                   clf_optimizer <span class="op">=</span> <span class="cn">NULL</span>,                   <span class="co"># classifiers optimizer</span>
                   trained <span class="op">=</span> <span class="cn">FALSE</span>,                        <span class="co"># indicates whether provided classifier is </span>
                                                           <span class="co"># trained</span>
                   
                   train_x <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_x</span>,                 <span class="co"># train predictors</span>
                   train_y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_y</span>,                 <span class="co"># train target</span>
                   sensitive_train <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_train</span>, <span class="co"># train sensitives</span>
                   sensitive_test <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_test</span>,   <span class="co"># test sensitives</span>
                   
                   batch_size <span class="op">=</span> <span class="fl">5</span>,                         <span class="co"># inner dataset_loader batch size</span>
                   partition <span class="op">=</span> <span class="fl">0.6</span>,                        <span class="co"># partition for inner adversaries </span>
                                                           <span class="co"># dataset_loader preparation</span>
                   
                   neurons_clf <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">32</span><span class="op">)</span>,            <span class="co"># classifiers neural architecture</span>
                   neurons_adv <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">32</span><span class="op">)</span>,            <span class="co"># adversaries neural architecture</span>
                   dimension_clf <span class="op">=</span> <span class="fl">2</span>,                      <span class="co"># dimension for classifier (always set 2)</span>
                   dimension_adv <span class="op">=</span> <span class="fl">1</span>,                      <span class="co"># dimension for adversarial (always set 1)</span>
                   learning_rate_clf <span class="op">=</span> <span class="fl">0.001</span>,              <span class="co"># learning rate of classifier</span>
                   learning_rate_adv <span class="op">=</span> <span class="fl">0.001</span>,              <span class="co"># learning rate of adversarial</span>
                   n_ep_preclf <span class="op">=</span> <span class="fl">10</span>,                       <span class="co"># number of epochs for classifier pretrain</span>
                   n_ep_preadv <span class="op">=</span> <span class="fl">10</span>,                       <span class="co"># number of epochs for adversarial pretrain</span>
                   
                   dsl <span class="op">=</span> <span class="va">dsl</span>,                              <span class="co"># dataset_loader</span>
                   dev <span class="op">=</span> <span class="va">dev</span>,                              <span class="co"># computational device</span>
                   verbose <span class="op">=</span> <span class="cn">TRUE</span>,                         <span class="co"># if TRUE prints metrics</span>
                   monitor <span class="op">=</span> <span class="cn">TRUE</span>                          <span class="co"># if TRUE aquires more data ( also to print)</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Preclassifier at epoch 1: training loss: 0.633, validation: 0.579, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 2: training loss: 0.554, validation: 0.523, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 3: training loss: 0.530, validation: 0.513, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 4: training loss: 0.517, validation: 0.503, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 5: training loss: 0.504, validation: 0.496, accuracy: 0.802, STPR: 0.000
## Preclassifier at epoch 6: training loss: 0.487, validation: 0.491, accuracy: 0.808, STPR: 0.277
## Preclassifier at epoch 7: training loss: 0.469, validation: 0.491, accuracy: 0.808, STPR: 0.325
## Preclassifier at epoch 8: training loss: 0.454, validation: 0.489, accuracy: 0.808, STPR: 0.397
## Preclassifier at epoch 9: training loss: 0.442, validation: 0.491, accuracy: 0.808, STPR: 0.397
## Preclassifier at epoch 10: training loss: 0.432, validation: 0.491, accuracy: 0.814, STPR: 0.415
## Preadversary at epoch 1: training loss: 0.693, validation: 0.692, accuracy: 0.644
## Preadversary at epoch 2: training loss: 0.692, validation: 0.692, accuracy: 0.644
## Preadversary at epoch 3: training loss: 0.690, validation: 0.690, accuracy: 0.644
## Preadversary at epoch 4: training loss: 0.687, validation: 0.688, accuracy: 0.654
## Preadversary at epoch 5: training loss: 0.684, validation: 0.686, accuracy: 0.635
## Preadversary at epoch 6: training loss: 0.682, validation: 0.685, accuracy: 0.635
## Preadversary at epoch 7: training loss: 0.680, validation: 0.685, accuracy: 0.635
## Preadversary at epoch 8: training loss: 0.679, validation: 0.685, accuracy: 0.644
## Preadversary at epoch 9: training loss: 0.678, validation: 0.685, accuracy: 0.654
## Preadversary at epoch 10: training loss: 0.677, validation: 0.685, accuracy: 0.644</code></pre>
<p>As we’ve mentioned before, we can also provide our own pretrained model to the <code>pretrain</code> function, just like in the example below. Note that in this particular case we create the same classifier as in the previous example and the only difference is that we do it outside the <code>pretrain</code> function. <code>create_model</code> and <code>pretrain_net</code> are the methods which we also use under the <code>pretrain</code>, but the user is also able to call them and their parameters are similar to those in the pretrain. <code>create_model</code> returns the neural network architecture, whereas <code>pretrain_net</code> proceeds with its training and returns losses and optimizer from the learning process.</p>
<p>As you can see, in <code>pretrain</code> we provide <code>clf_model</code>,<code>clf_optimizer</code> and change <code>trained</code> to TRUE as stated before.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">clf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_model.html">create_model</a></span><span class="op">(</span>train_x <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_x</span>,                <span class="co"># train predictors</span>
                    train_y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_y</span>,                <span class="co"># train target </span>
                    neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32</span>,<span class="fl">32</span>,<span class="fl">32</span><span class="op">)</span>,                 <span class="co"># models neural architecture</span>
                    dimensions <span class="op">=</span> <span class="fl">2</span>                         <span class="co"># dimension for model (always set 2 for </span>
                                                           <span class="co"># classifier 1 for adversary)</span>
<span class="op">)</span>

<span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pretrain_net.html">pretrain_net</a></span><span class="op">(</span>n_epochs <span class="op">=</span> <span class="fl">10</span>,                         <span class="co"># number of epochs for model pretrain</span>
                    model <span class="op">=</span> <span class="va">clf</span>,                           <span class="co"># neural network model</span>
                    dsl <span class="op">=</span> <span class="va">dsl</span>,                             <span class="co"># dataset_loader</span>
                    model_type <span class="op">=</span> <span class="fl">1</span>,                        <span class="co"># model type (1 means precalssifer)</span>
                    learning_rate <span class="op">=</span> <span class="fl">0.001</span>,                 <span class="co"># learning rate of classifier</span>
                    sensitive_test <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_test</span>,  <span class="co"># test sensitives</span>
                    dev <span class="op">=</span> <span class="va">dev</span>,                             <span class="co"># computational device</span>
                    verbose <span class="op">=</span> <span class="cn">TRUE</span>,                        <span class="co"># if TRUE prints metrics</span>
                    monitor <span class="op">=</span> <span class="cn">TRUE</span>                         <span class="co"># if TRUE aquires more data ( also to print)</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Preclassifier at epoch 1: training loss: 0.633, validation: 0.579, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 2: training loss: 0.554, validation: 0.523, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 3: training loss: 0.530, validation: 0.513, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 4: training loss: 0.517, validation: 0.503, accuracy: 0.785, STPR: NaN
## Preclassifier at epoch 5: training loss: 0.504, validation: 0.496, accuracy: 0.802, STPR: 0.000
## Preclassifier at epoch 6: training loss: 0.487, validation: 0.491, accuracy: 0.808, STPR: 0.277
## Preclassifier at epoch 7: training loss: 0.469, validation: 0.491, accuracy: 0.808, STPR: 0.325
## Preclassifier at epoch 8: training loss: 0.454, validation: 0.489, accuracy: 0.808, STPR: 0.397
## Preclassifier at epoch 9: training loss: 0.442, validation: 0.491, accuracy: 0.808, STPR: 0.397
## Preclassifier at epoch 10: training loss: 0.432, validation: 0.491, accuracy: 0.814, STPR: 0.415</code></pre>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">opt</span><span class="op">$</span><span class="va">optimizer</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;optim_adam&gt;
##   Inherits from: &lt;torch_Optimizer&gt;
##   Public:
##     add_param_group: function (param_group) 
##     clone: function (deep = FALSE) 
##     defaults: list
##     initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, 
##     load_state_dict: function (state_dict) 
##     param_groups: list
##     state: State, R6
##     state_dict: function () 
##     step: function (closure = NULL) 
##     zero_grad: function () 
##   Private:
##     step_helper: function (closure, loop_fun)</code></pre>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">clf_optimizer</span> <span class="op">&lt;-</span> <span class="va">opt</span><span class="op">$</span><span class="va">optimizer</span>
    
<span class="va">models</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pretrain.html">pretrain</a></span><span class="op">(</span>clf_model <span class="op">=</span> <span class="va">clf</span>,                        <span class="co"># classifier model</span>
                   adv_model <span class="op">=</span> <span class="cn">NULL</span>,                       <span class="co"># adversarial model</span>
                   clf_optimizer <span class="op">=</span> <span class="va">clf_optimizer</span>,          <span class="co"># classifiers optimizer</span>
                   trained <span class="op">=</span> <span class="cn">TRUE</span>,                         <span class="co"># indicates whether provided classifier is </span>
                                                           <span class="co"># trained</span>
                   train_x <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_x</span>,                 <span class="co"># train predictors</span>
                   train_y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">train_y</span>,                 <span class="co"># train target</span>
                   sensitive_train <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_train</span>, <span class="co"># train sensitives</span>
                   sensitive_test <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_test</span>,   <span class="co"># test sensitives</span>
                   batch_size <span class="op">=</span> <span class="fl">5</span>,                         <span class="co"># inner dataset_loader batch size</span>
                   partition <span class="op">=</span> <span class="fl">0.6</span>,                        <span class="co"># partition for inner adversaries </span>
                                                           <span class="co"># dataset_loader preparation</span>
                   neurons_clf <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">32</span><span class="op">)</span>,            <span class="co"># classifiers neural architecture</span>
                   neurons_adv <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">32</span><span class="op">)</span>,            <span class="co"># adversaries neural architecture</span>
                   dimension_clf <span class="op">=</span> <span class="fl">2</span>,                      <span class="co"># dimension for classifier (always set 2)</span>
                   dimension_adv <span class="op">=</span> <span class="fl">1</span>,                      <span class="co"># dimension for adversarial (always set 1)</span>
                   learning_rate_clf <span class="op">=</span> <span class="fl">0.001</span>,              <span class="co"># learning rate of classifier</span>
                   learning_rate_adv <span class="op">=</span> <span class="fl">0.001</span>,              <span class="co"># learning rate of adversarial</span>
                   n_ep_preclf <span class="op">=</span> <span class="fl">5</span>,                        <span class="co"># number of epochs for classifier pretrain</span>
                   n_ep_preadv <span class="op">=</span> <span class="fl">10</span>,                       <span class="co"># number of epochs for adversarial pretrain</span>
                   dsl <span class="op">=</span> <span class="va">dsl</span>,                              <span class="co"># dataset_loader</span>
                   dev <span class="op">=</span> <span class="va">dev</span>,                              <span class="co"># computational device</span>
                   verbose <span class="op">=</span> <span class="cn">TRUE</span>,                         <span class="co"># if TRUE prints metrics</span>
                   monitor <span class="op">=</span> <span class="cn">TRUE</span>                          <span class="co"># if TRUE aquires more data ( also to print)</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Preadversary at epoch 1: training loss: 0.693, validation: 0.692, accuracy: 0.644
## Preadversary at epoch 2: training loss: 0.692, validation: 0.692, accuracy: 0.644
## Preadversary at epoch 3: training loss: 0.690, validation: 0.690, accuracy: 0.644
## Preadversary at epoch 4: training loss: 0.687, validation: 0.688, accuracy: 0.654
## Preadversary at epoch 5: training loss: 0.684, validation: 0.686, accuracy: 0.635
## Preadversary at epoch 6: training loss: 0.682, validation: 0.685, accuracy: 0.635
## Preadversary at epoch 7: training loss: 0.680, validation: 0.685, accuracy: 0.635
## Preadversary at epoch 8: training loss: 0.679, validation: 0.685, accuracy: 0.644
## Preadversary at epoch 9: training loss: 0.678, validation: 0.685, accuracy: 0.654
## Preadversary at epoch 10: training loss: 0.677, validation: 0.685, accuracy: 0.644</code></pre>
<p>In the end we want to create an explainer for our classification model, but we will say more about it later.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">exp_clf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain_pan.html">explain_pan</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">test_y</span>,                          <span class="co"># test target</span>
                       model <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">clf_model</span>,                 <span class="co"># classifier model</span>
                       label <span class="op">=</span> <span class="st">"Classifier"</span>,                     <span class="co"># classifiers name</span>
                       original_data <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">data_test</span>,           <span class="co"># original data for test</span>
                       data <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">data_scaled_test</span>,             <span class="co"># scaled numeric data for test</span>
                       batch_size <span class="op">=</span> <span class="fl">5</span>,                           <span class="co"># batch_size used in dataset_loader</span>
                       dev <span class="op">=</span> <span class="va">dev</span>,                                <span class="co"># computational device</span>
                       verbose <span class="op">=</span> <span class="cn">TRUE</span>                            <span class="co"># if TRUE prints monitor info</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Preparation of a new explainer is initiated
##   -&gt; model label       :  Classifier 
##   -&gt; data              :  172  rows  15  cols 
##   -&gt; target variable   :  172  values 
##   -&gt; predict function  :  custom_predict 
##   -&gt; predicted values  :  No value for predict function target column. ( [33m default [39m )
##   -&gt; model_info        :  package Model of class: net package unrecognized , ver. Unknown , task regression ( [33m default [39m ) 
##   -&gt; model_info        :  type set to  classification 
##   -&gt; predicted values  :  numerical, min =  4.826793e-12 , mean =  0.2093583 , max =  0.9950494  
##   -&gt; residual function :  difference between y and yhat ( [33m default [39m )
##   -&gt; residuals         :  numerical, min =  -0.9874027 , mean =  0.005757974 , max =  0.9999998  
##  [32m A new explainer has been created! [39m</code></pre>
</div>
<div id="fairtrain" class="section level2">
<h2 class="hasAnchor">
<a href="#fairtrain" class="anchor"></a>Fairtrain</h2>
<p>Finally we’ve reached the most important part of this package which is the <code>fair_train</code> function. This function enables us to conduct a fair training which optimizes Statistical Parity Ratio, thus leading to similar distributions of privileged and discriminated groups. As a short reminder, this method is based on a mutual training of the classifier and adversarial, where classifier learns to deceive the adversarial, so in the end it cannot predict the correct label. As we use previous models we need to provide them with correct <code>optimizers</code>. Moreover in the loss function equation we see a <code>lambda</code> parameter which we also have to provide. The intuition behind it states that the bigger the <code>lambda</code> is the fairer the training is, but traditional performance, like accuracy worsens faster. At this point you should be familiar with rest of the parameters, however <code>monitor</code> here collects much more training data (STP ratio, adversary accuracy, adversary loss and classifier accuracy) which can be used to monitor the training process. On the other hand calculating this metrics takes lots of time so turning <code>monitor</code> off is good when you’re sure that your model works properly. This function returns NULL if <code>monitor</code> is off and aforementioned metrics if it is on.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">monitor</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fair_train.html">fair_train</a></span><span class="op">(</span> n_ep_pan <span class="op">=</span> <span class="fl">17</span>,                           <span class="co"># number of epochs for pan training</span>
                       dsl <span class="op">=</span> <span class="va">dsl</span>,                               <span class="co"># dataset_loader</span>
                       
                       clf_model <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">clf_model</span>,            <span class="co"># classifier model</span>
                       adv_model <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">adv_model</span>,            <span class="co"># adv model</span>
                       clf_optimizer <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">clf_optimizer</span>,    <span class="co"># classifiers optimizer</span>
                       adv_optimizer <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">adv_optimizer</span>,    <span class="co"># adversaries optimizer</span>
                       
                       dev <span class="op">=</span> <span class="va">dev</span>,                               <span class="co"># computational device</span>
                       sensitive_train <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_train</span>,  <span class="co"># train sensitives</span>
                       sensitive_test <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">sensitive_test</span>,    <span class="co"># test sensitives</span>
                       
                       batch_size <span class="op">=</span> <span class="fl">5</span>,                          <span class="co"># inner dataset_loader batch size</span>
                       learning_rate_adv <span class="op">=</span> <span class="fl">0.001</span>,               <span class="co"># learning rate of adversarial</span>
                       learning_rate_clf <span class="op">=</span> <span class="fl">0.001</span>,               <span class="co"># learning rate of classifier</span>
                       lambda <span class="op">=</span> <span class="fl">130</span>,                            <span class="co"># train controlling parameter (the </span>
                                                                <span class="co"># bigger the better STPR results)</span>
                       
                       verbose <span class="op">=</span> <span class="cn">TRUE</span>,                         <span class="co"># if TRUE prints metrics</span>
                       monitor <span class="op">=</span> <span class="cn">TRUE</span>                           <span class="co"># if TRUE training collects 4 metrics </span>
                                                                <span class="co"># throughout the epochs</span>
<span class="op">)</span></code></pre></div>
<pre><code>## PAN epoch 1 
## Classifier at epoch 1:training loss: -99.599, accuracy: 0.802
## Adversary at epoch 1: training loss: 88.542,accuracy: 0.624, STPR: 0.433
## PAN epoch 2 
## Classifier at epoch 2:training loss: -74.199, accuracy: 0.802
## Adversary at epoch 2: training loss: 88.366,accuracy: 0.616, STPR: 0.433
## PAN epoch 3 
## Classifier at epoch 3:training loss: -71.105, accuracy: 0.808
## Adversary at epoch 3: training loss: 88.725,accuracy: 0.601, STPR: 0.435
## PAN epoch 4 
## Classifier at epoch 4:training loss: -86.280, accuracy: 0.814
## Adversary at epoch 4: training loss: 89.221,accuracy: 0.593, STPR: 0.456
## PAN epoch 5 
## Classifier at epoch 5:training loss: -91.172, accuracy: 0.808
## Adversary at epoch 5: training loss: 89.643,accuracy: 0.593, STPR: 0.480
## PAN epoch 6 
## Classifier at epoch 6:training loss: -94.640, accuracy: 0.808
## Adversary at epoch 6: training loss: 89.867,accuracy: 0.566, STPR: 0.480
## PAN epoch 7 
## Classifier at epoch 7:training loss: -90.038, accuracy: 0.814
## Adversary at epoch 7: training loss: 90.103,accuracy: 0.550, STPR: 0.437
## PAN epoch 8 
## Classifier at epoch 8:training loss: -84.004, accuracy: 0.808
## Adversary at epoch 8: training loss: 90.144,accuracy: 0.547, STPR: 0.519
## PAN epoch 9 
## Classifier at epoch 9:training loss: -93.248, accuracy: 0.802
## Adversary at epoch 9: training loss: 90.235,accuracy: 0.543, STPR: 0.553
## PAN epoch 10 
## Classifier at epoch 10:training loss: -89.786, accuracy: 0.802
## Adversary at epoch 10: training loss: 90.238,accuracy: 0.539, STPR: 0.553
## PAN epoch 11 
## Classifier at epoch 11:training loss: -90.503, accuracy: 0.802
## Adversary at epoch 11: training loss: 90.213,accuracy: 0.535, STPR: 0.553
## PAN epoch 12 
## Classifier at epoch 12:training loss: -90.106, accuracy: 0.791
## Adversary at epoch 12: training loss: 90.156,accuracy: 0.523, STPR: 0.652
## PAN epoch 13 
## Classifier at epoch 13:training loss: -89.872, accuracy: 0.791
## Adversary at epoch 13: training loss: 90.085,accuracy: 0.601, STPR: 0.652
## PAN epoch 14 
## Classifier at epoch 14:training loss: -89.116, accuracy: 0.791
## Adversary at epoch 14: training loss: 90.018,accuracy: 0.484, STPR: 0.761
## PAN epoch 15 
## Classifier at epoch 15:training loss: -92.320, accuracy: 0.791
## Adversary at epoch 15: training loss: 89.941,accuracy: 0.484, STPR: 0.761
## PAN epoch 16 
## Classifier at epoch 16:training loss: -89.222, accuracy: 0.791
## Adversary at epoch 16: training loss: 89.878,accuracy: 0.488, STPR: 0.761
## PAN epoch 17 
## Classifier at epoch 17:training loss: -89.998, accuracy: 0.791
## Adversary at epoch 17: training loss: 89.763,accuracy: 0.496, STPR: 0.905</code></pre>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">monitor</span></code></pre></div>
<pre><code>## $STP
##  [1] 0.4329325 0.4329325 0.4346505 0.4563830 0.4804031 0.4804031 0.4367301
##  [8] 0.5186170 0.5531915 0.5531915 0.5531915 0.6519757 0.6519757 0.7606383
## [15] 0.7606383 0.7606383 0.9052224
## 
## $adversary_acc
##  [1] 0.6240310 0.6162791 0.6007752 0.5930233 0.5930233 0.5658915 0.5503876
##  [8] 0.5465116 0.5426357 0.5387597 0.5348837 0.5232558 0.6007752 0.4844961
## [15] 0.4844961 0.4883721 0.4961240
## 
## $classifier_acc
##  [1] 0.8023256 0.8023256 0.8081395 0.8139535 0.8081395 0.8081395 0.8139535
##  [8] 0.8081395 0.8023256 0.8023256 0.8023256 0.7906977 0.7906977 0.7906977
## [15] 0.7906977 0.7906977 0.7906977
## 
## $adversary_losses
##  [1] 88.54218 88.36598 88.72516 89.22082 89.64261 89.86734 90.10343 90.14378
##  [9] 90.23547 90.23825 90.21337 90.15602 90.08535 90.01792 89.94072 89.87830
## [17] 89.76320</code></pre>
</div>
<div id="metrics-and-visualizations" class="section level2">
<h2 class="hasAnchor">
<a href="#metrics-and-visualizations" class="anchor"></a>Metrics and Visualizations</h2>
<p>After the training process we’d like to visualize the final outcome and the monitor of metrics throughout learning. To achieve that we have to use <code>monitor</code> in <code>fair_train</code> and apply <code>plot_monitor</code> function to it. This function is especially convenient in the process of parameters tuning and model adjustments, because one can easily check if the model learns properly.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/plot_monitor.html">plot_monitor</a></span><span class="op">(</span>STP <span class="op">=</span> <span class="va">monitor</span><span class="op">$</span><span class="va">STP</span>,                                  <span class="co"># monitor for STP</span>
             adversary_acc <span class="op">=</span> <span class="va">monitor</span><span class="op">$</span><span class="va">adversary_acc</span>,              <span class="co"># monitor for adversaries accuracy</span>
             adversary_losses <span class="op">=</span> <span class="va">monitor</span><span class="op">$</span><span class="va">adversary_losses</span>,        <span class="co"># monitor for adversaries loss</span>
             classifier_acc <span class="op">=</span> <span class="va">monitor</span><span class="op">$</span><span class="va">classifier_acc</span><span class="op">)</span>            <span class="co"># monitor for classifiers accuracy</span></code></pre></div>
<p><img src="Tutorial_files/figure-html/plot_monitor-1.png" width="700"><img src="Tutorial_files/figure-html/plot_monitor-2.png" width="700"><img src="Tutorial_files/figure-html/plot_monitor-3.png" width="700"><img src="Tutorial_files/figure-html/plot_monitor-4.png" width="700"></p>
<p>As fairness is closely connected with XAI, we also propose an option to create an explainer for our neural network models with <code>explain_pan</code> which is <code><a href="https://dalex.drwhy.ai/reference/explain.html">DALEX::explain</a></code> function with proper adjustments for neural network models.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">exp_PAN</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain_pan.html">explain_pan</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">test_y</span>,                          <span class="co"># test target</span>
                       model <span class="op">=</span> <span class="va">models</span><span class="op">$</span><span class="va">clf_model</span>,                 <span class="co"># classifier model</span>
                       label <span class="op">=</span> <span class="st">"PAN"</span>,                            <span class="co"># classifiers name</span>
                       original_data <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">data_test</span>,           <span class="co"># original data for test</span>
                       data <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">data_scaled_test</span>,             <span class="co"># scaled numeric data for test</span>
                       batch_size <span class="op">=</span> <span class="fl">5</span>,                           <span class="co"># batch_size used in dataset_loader</span>
                       dev <span class="op">=</span> <span class="va">dev</span>,                                <span class="co"># computational device</span>
                       verbose <span class="op">=</span> <span class="cn">TRUE</span>                            <span class="co"># if TRUE prints additional info</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Preparation of a new explainer is initiated
##   -&gt; model label       :  PAN 
##   -&gt; data              :  172  rows  15  cols 
##   -&gt; target variable   :  172  values 
##   -&gt; predict function  :  custom_predict 
##   -&gt; predicted values  :  No value for predict function target column. ( [33m default [39m )
##   -&gt; model_info        :  package Model of class: net package unrecognized , ver. Unknown , task regression ( [33m default [39m ) 
##   -&gt; model_info        :  type set to  classification 
##   -&gt; predicted values  :  numerical, min =  2.696841e-14 , mean =  0.1481732 , max =  0.9959807  
##   -&gt; residual function :  difference between y and yhat ( [33m default [39m )
##   -&gt; residuals         :  numerical, min =  -0.9854026 , mean =  0.06694311 , max =  1  
##  [32m A new explainer has been created! [39m</code></pre>
<p>All functions that work on DALEX explainer object also applies to our <code>explain_pan</code>, but probably two most interesting ones are <code>model_performance</code> and <code><a href="https://rdrr.io/pkg/fairmodels/man/fairness_check.html">fairmodels::fairness_check</a></code>.</p>
<p>The first one proves useful when we want to see the performance of the model. We’d like to show you this functionality by comparing performance of PAN and ordinary classification model. As we can see below it is true that performance of PAN slightly diminished (but it’s only 1.2% on accuracy!), but the next plot will show us why it is okay for us.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">DALEX</span><span class="fu">::</span><span class="fu"><a href="https://dalex.drwhy.ai/reference/model_performance.html">model_performance</a></span><span class="op">(</span><span class="va">exp_PAN</span><span class="op">)</span></code></pre></div>
<pre><code>## Measures for:  classification
## recall     : 0.3243243 
## precision  : 0.5217391 
## f1         : 0.4 
## accuracy   : 0.7906977 
## auc        : 0.8114114
## 
## Residuals:
##            0%           10%           20%           30%           40% 
## -9.854026e-01 -1.366224e-01 -3.940820e-03 -8.915482e-05 -4.210485e-06 
##           50%           60%           70%           80%           90% 
## -4.095541e-07 -3.830160e-08 -1.062288e-09  2.399297e-02  9.364332e-01 
##          100% 
##  1.000000e+00</code></pre>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">DALEX</span><span class="fu">::</span><span class="fu"><a href="https://dalex.drwhy.ai/reference/model_performance.html">model_performance</a></span><span class="op">(</span><span class="va">exp_clf</span><span class="op">)</span></code></pre></div>
<pre><code>## Measures for:  classification
## recall     : 0.5135135 
## precision  : 0.5757576 
## f1         : 0.5428571 
## accuracy   : 0.8139535 
## auc        : 0.8266266
## 
## Residuals:
##            0%           10%           20%           30%           40% 
## -9.874027e-01 -3.375116e-01 -3.382319e-02 -9.803546e-04 -8.439770e-05 
##           50%           60%           70%           80%           90% 
## -9.038159e-06 -1.157365e-06 -3.813997e-08  1.248153e-02  5.808426e-01 
##          100% 
##  9.999998e-01</code></pre>
<p>Finally, with <code><a href="https://rdrr.io/pkg/fairmodels/man/fairness_check.html">fairmodels::fairness_check</a></code> used on explainers (<code>exp_PAN</code>, <code>exp_clf</code>) we can compare fairness metrics for both models. We can use this function on a single or many explainers, but we have to provide them after comma one by one and ensure that they have different labels. After creating fairness object (<code>fobject</code>) we can plot it with regular <code>plot</code> function.</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fobject</span> <span class="op">&lt;-</span> <span class="fu">fairmodels</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/fairmodels/man/fairness_check.html">fairness_check</a></span><span class="op">(</span><span class="va">exp_PAN</span>,<span class="va">exp_clf</span>,        <span class="co"># explainers given one by one</span>
                            protected <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">protected_test</span>,  <span class="co"># protected column</span>
                            privileged <span class="op">=</span> <span class="st">"Male"</span>,              <span class="co"># privileged class from protected</span>
                            verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>                   <span class="co"># if TRUE prints additional info</span></code></pre></div>
<pre><code>## Creating fairness classification object
## -&gt; Privileged subgroup       : character ([32m Ok [39m )
## -&gt; Protected variable        : factor ([32m Ok [39m ) 
## -&gt; Cutoff values for explainers  : 0.5 ( for all subgroups ) 
## -&gt; Fairness objects      : 0 objects 
## -&gt; Checking explainers       : 2 in total ( [32m compatible [39m )
## -&gt; Metric calculation        : 12/12 metrics calculated for all models
## [32m Fairness object created succesfully [39m</code></pre>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fobject</span><span class="op">)</span></code></pre></div>
<p><img src="Tutorial_files/figure-html/fairness_check-1.png" width="700"></p>
<p>As we can see, Statistical Parity ratio dramatically increased and the model is almost fair according to this metric. On the other hand we see worsening of other metrics, like Equal Opportunity ratio, however it can be easily explained. During the fair_train process we make more TP (True Positives) and FP (False Positives) which directly changes the value of Equal opportunity ratio, so this sacrifice is unavoidable.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Hubert Ruczyński, Jakub Wiśniewski.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
